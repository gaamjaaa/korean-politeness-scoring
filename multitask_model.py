import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, BertTokenizer
import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import StratifiedKFold
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import random
from sklearn.metrics import classification_report, mean_absolute_error

# FEATURE_CONFIGS import
FEATURE_CONFIGS = {
    'feat_ending': 3,      # 0, 1, 2 (ì›ë˜ 3ì´ 2ë¡œ ë³‘í•©)
    'feat_strat_cnt': 3,   # 0, 1, 2 (ë³€í™” ì—†ìŒ)
    'feat_command': 3,     # 0, 1, 2 (ì›ë˜ 3ì´ 2ë¡œ ë³‘í•©)
    'feat_attack': 3,      # 0, 1, 2 (ì›ë˜ 3ì´ 2ë¡œ ë³‘í•©)
    'feat_power': 3,       # 0, 1, 2 (ì›ë˜ 3ì´ 2ë¡œ ë³‘í•©)
    'feat_distance': 3,    # 0, 1, 2 (ì›ë˜ 3ì´ 2ë¡œ ë³‘í•©)
    'feat_indirect': 3     # 0, 1, 2 (ë³€í™” ì—†ìŒ)
}

class KoPolitenessDataset(Dataset):
    """í•œêµ­ì–´ ê³µì†ë„ ë°ì´í„°ì…‹"""
    def __init__(self, sentences, feat_labels=None, scores=None, tokenizer=None, max_length=512):
        self.sentences = sentences
        self.feat_labels = feat_labels  # Manual ë°ì´í„°: [N, 7], TyDiP: None
        self.scores = scores
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # í”¼ì²˜ ì´ë¦„ ì •ì˜
        self.feature_names = [
            "feat_ending", "feat_strat_cnt", "feat_command",
            "feat_attack", "feat_power", "feat_distance", "feat_indirect"
        ]
        
    def __len__(self):
        return len(self.sentences)
    
    def __getitem__(self, idx):
        sentence = str(self.sentences[idx])
        
        # í† í¬ë‚˜ì´ì§•
        encoded = self.tokenizer(
            sentence,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        item = {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze()
        }
        
        # í”¼ì²˜ ë¼ë²¨ (Manual ë°ì´í„°ë§Œ)
        if self.feat_labels is not None:
            feat_labels_tensor = torch.tensor(self.feat_labels[idx], dtype=torch.long)
            feat_mask = torch.ones(7, dtype=torch.bool)  # Manual ë°ì´í„°ëŠ” ëª¨ë“  í”¼ì²˜ ì‚¬ìš©
        else:
            feat_labels_tensor = torch.tensor([-1] * 7, dtype=torch.long)  # TyDiP ë°ì´í„°
            feat_mask = torch.zeros(7, dtype=torch.bool)  # í”¼ì²˜ ë§ˆìŠ¤í‚¹
        
        item['feat_labels'] = feat_labels_tensor
        item['feat_mask'] = feat_mask
        
        # ìŠ¤ì½”ì–´
        if self.scores is not None:
            item['score'] = torch.tensor(self.scores[idx], dtype=torch.float)
            item['score_mask'] = torch.tensor(True, dtype=torch.bool)
        else:
            item['score'] = torch.tensor(0.0, dtype=torch.float)
            item['score_mask'] = torch.tensor(False, dtype=torch.bool)
            
        return item

class ImprovedMultiTaskModel(nn.Module):
    """ê°œì„ ëœ ë©€í‹°íƒœìŠ¤í¬ ê³µì†ë„ ì˜ˆì¸¡ ëª¨ë¸"""
    def __init__(self, model_name='monologg/kobert', dropout_rate=0.3):
        super().__init__()
        
        # KoBERT ì¸ì½”ë”
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.bert.config.hidden_size  # 768
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout_rate)
        
        # í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµì„ ìœ„í•œ ê³µí†µ ë ˆì´ì–´
        self.feature_interaction = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(self.hidden_size // 2, self.hidden_size // 2)
        )
        
        # ê° í”¼ì²˜ë³„ ë¶„ë¥˜ í—¤ë“œ (ì—…ë°ì´íŠ¸ëœ í´ë˜ìŠ¤ ìˆ˜ ì‚¬ìš©)
        self.feature_heads = nn.ModuleDict({
            "feat_ending": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_ending']),
            "feat_strat_cnt": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_strat_cnt']),
            "feat_command": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_command']),
            "feat_attack": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_attack']),
            "feat_power": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_power']),
            "feat_distance": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_distance']),
            "feat_indirect": nn.Linear(self.hidden_size // 2, FEATURE_CONFIGS['feat_indirect'])
        })
        
        # ìµœì¢… ì ìˆ˜ íšŒê·€ í—¤ë“œ (ì—…ë°ì´íŠ¸ëœ ì°¨ì› ìˆ˜ì •)
        # BERT features (768) + feature predictions (3*7 = 21)
        feature_pred_size = sum(FEATURE_CONFIGS.values())  # ëª¨ë“  í”¼ì²˜ì˜ í´ë˜ìŠ¤ ìˆ˜ í•©
        self.score_head = nn.Sequential(
            nn.Linear(self.hidden_size + feature_pred_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 1)
        )
        
    def forward(self, input_ids, attention_mask):
        # BERT ì¸ì½”ë”©
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = bert_output.pooler_output  # [batch_size, hidden_size]
        pooled_output = self.dropout(pooled_output)
        
        # í”¼ì²˜ ìƒí˜¸ì‘ìš© í•™ìŠµ
        interaction_features = self.feature_interaction(pooled_output)
        
        # ê° í”¼ì²˜ë³„ ì˜ˆì¸¡
        feature_logits = {}
        feature_probs = {}
        
        for feat_name, head in self.feature_heads.items():
            logits = head(interaction_features)
            feature_logits[feat_name] = logits
            feature_probs[feat_name] = F.softmax(logits, dim=-1)
        
        # ìµœì¢… ì ìˆ˜ ì˜ˆì¸¡ (BERT features + feature predictions ê²°í•©)
        all_probs = torch.cat(list(feature_probs.values()), dim=-1)
        score_input = torch.cat([pooled_output, all_probs], dim=-1)
        score_output = self.score_head(score_input)
        
        return feature_logits, score_output.squeeze(-1)

class FocalLoss(nn.Module):
    """Focal Loss for imbalanced classification"""
    def __init__(self, alpha=1, gamma=2, weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.weight = weight
        
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

def compute_class_weights(labels, num_classes, feature_name=None):
    """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì¤‘ìš” í´ë˜ìŠ¤ ë¶€ìŠ¤íŠ¸ í¬í•¨)"""
    unique_labels = np.unique(labels)
    class_weights = compute_class_weight(
        'balanced', 
        classes=unique_labels, 
        y=labels
    )
    
    # ëˆ„ë½ëœ í´ë˜ìŠ¤ì— ëŒ€í•´ ê¸°ë³¸ê°’ ì„¤ì •
    weights = np.ones(num_classes)
    for i, label in enumerate(unique_labels):
        weights[label] = class_weights[i]
    
    # ì¤‘ìš”í•œ í¬ê·€ í´ë˜ìŠ¤ì— ì¶”ê°€ ë¶€ìŠ¤íŠ¸ ì ìš© (í´ë˜ìŠ¤ 3 â†’ 2 ë³‘í•© ë°˜ì˜)
    if feature_name == 'feat_attack':
        # feat_attack class 2 (í´ë˜ìŠ¤ 3ì´ ë³‘í•©ë¨)ëŠ” ê³µì†ë„ì— ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ ê°•í•˜ê²Œ ë¶€ìŠ¤íŠ¸
        if 2 < len(weights):
            weights[2] *= 4.0  # í´ë˜ìŠ¤ 3ì˜ ì¤‘ìš”ë„ë¥¼ í´ë˜ìŠ¤ 2ì— ë°˜ì˜
            print(f"ğŸš€ Boosted {feature_name} class 2 weight: {weights[2]:.1f}")
            
        # class 1ë„ ë¶€ìŠ¤íŠ¸ (í¬ê·€í•˜ë¯€ë¡œ)
        if 1 < len(weights):
            weights[1] *= 2.5
            print(f"ğŸ”§ Boosted {feature_name} class 1 weight: {weights[1]:.1f}")
    
    elif feature_name == 'feat_distance':
        # feat_distance class 2 (í´ë˜ìŠ¤ 3ì´ ë³‘í•©ë¨)ë„ í¬ê·€í•˜ê³  ì¤‘ìš”
        if 2 < len(weights):
            weights[2] *= 3.0
            print(f"ğŸ”§ Boosted {feature_name} class 2 weight: {weights[2]:.1f}")
    
    elif feature_name == 'feat_command':
        # feat_command class 2 (í´ë˜ìŠ¤ 3ì´ ë³‘í•©ë¨)ë„ ê³µì†ë„ì— ì¤‘ìš”
        if 2 < len(weights):
            weights[2] *= 3.0
            print(f"ğŸ”§ Boosted {feature_name} class 2 weight: {weights[2]:.1f}")
        
        # class 1ë„ ë¶€ìŠ¤íŠ¸
        if 1 < len(weights):
            weights[1] *= 1.5
            print(f"ğŸ”§ Boosted {feature_name} class 1 weight: {weights[1]:.1f}")
    
    elif feature_name == 'feat_ending':
        # feat_ending class 2 (í´ë˜ìŠ¤ 3ì´ ë³‘í•©ë¨)ë„ í¬ê·€í•˜ë¯€ë¡œ ë¶€ìŠ¤íŠ¸
        if 2 < len(weights):
            weights[2] *= 2.5
            print(f"ğŸ”§ Boosted {feature_name} class 2 weight: {weights[2]:.1f}")
    
    elif feature_name == 'feat_power':
        # feat_power class 2 (í´ë˜ìŠ¤ 3ì´ ë³‘í•©ë¨)ë„ ë¶€ìŠ¤íŠ¸
        if 2 < len(weights):
            weights[2] *= 2.0
            print(f"ğŸ”§ Boosted {feature_name} class 2 weight: {weights[2]:.1f}")
    
    return torch.FloatTensor(weights)

def smart_train_val_split(df, test_size=0.2, min_rare_samples=2, random_state=42):
    """í¬ê·€ í´ë˜ìŠ¤ ê°•ì œë°°ì¹˜ë¥¼ í†µí•œ ìŠ¤ë§ˆíŠ¸ ë¶„í• """
    np.random.seed(random_state)
    
    feature_cols = [
        "feat_ending", "feat_strat_cnt", "feat_command",
        "feat_attack", "feat_power", "feat_distance", "feat_indirect"
    ]
    
    train_indices = []
    val_indices = []
    forced_allocation = {}  # ê°•ì œ ë°°ì¹˜ëœ ìƒ˜í”Œ ì¶”ì 
    
    print("ğŸ”§ Forced allocation for rare classes:")
    
    # 1ë‹¨ê³„: ê·¹í¬ê·€ í´ë˜ìŠ¤ ê°•ì œ ë°°ì¹˜ (< 10ê°œ)
    for col in feature_cols:
        value_counts = df[col].value_counts()
        
        for class_val, count in value_counts.items():
            class_indices = df[df[col] == class_val].index.tolist()
            
            if count < 10:  # ê·¹í¬ê·€ í´ë˜ìŠ¤
                # ìµœì†Œ 2ê°œëŠ” train, 1ê°œëŠ” valì— ê°•ì œ ë°°ì¹˜
                min_train = max(2, count - 2)
                min_val = min(1, count - min_train)
                
                np.random.shuffle(class_indices)
                
                forced_train = class_indices[:min_train]
                forced_val = class_indices[min_train:min_train + min_val]
                
                train_indices.extend(forced_train)
                val_indices.extend(forced_val)
                
                # ì¶”ì  ì •ë³´ ì €ì¥
                forced_allocation[f"{col}_{class_val}"] = {
                    'total': count,
                    'train': len(forced_train),
                    'val': len(forced_val)
                }
                
                print(f"  {col} class {class_val}: {count} samples â†’ train: {len(forced_train)}, val: {len(forced_val)}")
    
    # ì¤‘ë³µ ì œê±°
    val_indices = list(set(val_indices))
    train_indices = list(set(train_indices))
    
    print(f"ğŸ¯ Forced allocation completed: {len(train_indices)} train, {len(val_indices)} val")
    
    # 2ë‹¨ê³„: ë‚¨ì€ ìƒ˜í”Œë“¤ ì¸µí™” ì¶”ì¶œ
    remaining_indices = [idx for idx in df.index if idx not in train_indices and idx not in val_indices]
    
    if remaining_indices:
        remaining_df = df.loc[remaining_indices]
        
        # ë‹¤ì¤‘ í”¼ì²˜ ê¸°ë°˜ ë³µí•© ì¸µí™”
        # ì£¼ìš” í”¼ì²˜ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì¸µí™” íƒ€ê²Ÿ ìƒì„±
        primary_features = ['feat_ending', 'feat_attack', 'feat_power']
        stratify_labels = []
        
        for idx in remaining_indices:
            row = df.loc[idx]
            # ì£¼ìš” í”¼ì²˜ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì¸µí™” í‚¤ ìƒì„±
            key = f"{row['feat_ending']}-{row['feat_attack']}-{row['feat_power']}"
            stratify_labels.append(key)
        
        # ë„ˆë¬´ ì ì€ ê·¸ë£¹ë“¤ì€ í†µí•©
        from collections import Counter
        label_counts = Counter(stratify_labels)
        
        # 3ê°œ ë¯¸ë§Œì¸ ê·¸ë£¹ë“¤ì„ 'other'ë¡œ í†µí•©
        final_labels = []
        for label in stratify_labels:
            if label_counts[label] >= 3:
                final_labels.append(label)
            else:
                final_labels.append('other')
        
        try:
            from sklearn.model_selection import train_test_split
            remain_train, remain_val = train_test_split(
                remaining_indices,
                test_size=test_size,
                stratify=final_labels,
                random_state=random_state
            )
            
            train_indices.extend(remain_train)
            val_indices.extend(remain_val)
            
        except ValueError as e:
            print(f"âš ï¸  Stratification failed: {e}")
            # ì¸µí™” ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ëœë¤ ë¶„í• 
            np.random.shuffle(remaining_indices)
            split_point = int(len(remaining_indices) * (1 - test_size))
            train_indices.extend(remaining_indices[:split_point])
            val_indices.extend(remaining_indices[split_point:])
    
    print(f"ğŸ“Š Final split: {len(train_indices)} train, {len(val_indices)} val")
    return train_indices, val_indices

def forced_kfold_split(df, n_folds=3, random_state=42):
    """í¬ê·€ í´ë˜ìŠ¤ ê°•ì œë°°ì¹˜ K-fold"""
    np.random.seed(random_state)
    
    feature_cols = [
        "feat_ending", "feat_strat_cnt", "feat_command",
        "feat_attack", "feat_power", "feat_distance", "feat_indirect"
    ]
    
    # ê° fold ì´ˆê¸°í™”
    folds = [[] for _ in range(n_folds)]
    allocated_indices = set()
    
    print(f"ğŸ”§ Creating {n_folds}-fold splits with forced rare class allocation:")
    
    # 1ë‹¨ê³„: í¬ê·€ í´ë˜ìŠ¤ ê°•ì œ ë°°ì¹˜
    for col in feature_cols:
        value_counts = df[col].value_counts()
        
        for class_val, count in value_counts.items():
            if count < 15:  # í¬ê·€ í´ë˜ìŠ¤ ê¸°ì¤€
                class_indices = df[df[col] == class_val].index.tolist()
                class_indices = [idx for idx in class_indices if idx not in allocated_indices]
                
                if len(class_indices) == 0:
                    continue
                
                np.random.shuffle(class_indices)
                
                # ê° foldì— ìµœì†Œ 1ê°œì”© ë°°ì¹˜
                for fold_idx in range(min(n_folds, len(class_indices))):
                    if fold_idx < len(class_indices):
                        folds[fold_idx].append(class_indices[fold_idx])
                        allocated_indices.add(class_indices[fold_idx])
                
                # ë‚¨ì€ ìƒ˜í”Œë“¤ì„ foldì— ìˆœí™˜ ë°°ì¹˜
                remaining = class_indices[n_folds:]
                for i, idx in enumerate(remaining):
                    fold_idx = i % n_folds
                    folds[fold_idx].append(idx)
                    allocated_indices.add(idx)
                
                print(f"  {col} class {class_val}: {count} samples distributed across folds")
    
    # 2ë‹¨ê³„: ë‚¨ì€ ìƒ˜í”Œë“¤ ê· ë“± ë°°ì¹˜
    remaining_indices = [idx for idx in df.index if idx not in allocated_indices]
    np.random.shuffle(remaining_indices)
    
    # ê°€ì¥ ì‘ì€ foldë¶€í„° ìˆœí™˜í•˜ë©° ë°°ì¹˜
    for i, idx in enumerate(remaining_indices):
        # fold í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì„œ ì‘ì€ foldë¶€í„° ì±„ì›€
        fold_sizes = [(len(fold), fold_idx) for fold_idx, fold in enumerate(folds)]
        fold_sizes.sort()
        smallest_fold_idx = fold_sizes[0][1]
        folds[smallest_fold_idx].append(idx)
    
    # ê²°ê³¼ ì¶œë ¥
    print("ğŸ“Š Final fold distribution:")
    for i, fold in enumerate(folds):
        print(f"  Fold {i+1}: {len(fold)} samples")
    
    return folds

def compute_multitask_loss(feature_logits, score_pred, feat_labels, score_labels, 
                          feat_mask, score_mask, class_weights_dict, use_focal=True, alpha=2, gamma=3):
    """ë©€í‹°íƒœìŠ¤í¬ ì†ì‹¤ ê³„ì‚° (í”¼ì²˜ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ í¬í•¨)"""
    total_loss = 0.0
    feature_names = list(feature_logits.keys())
    
    # í”¼ì²˜ë³„ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ (ê³µì†ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ê¸°ì¤€)
    feature_importance = {
        'feat_attack': 3.5,    # ê³µê²©ì„±ì´ ê°€ì¥ ì¤‘ìš” (ì¦ê°€)
        'feat_command': 2.0,   # ëª…ë ¹ì„±ë„ ë§¤ìš° ì¤‘ìš” (ì¦ê°€)
        'feat_ending': 3.0,    # ì–´ë¯¸ë„ ì¤‘ìš” (ì¦ê°€)
        'feat_distance': 2.5,  # ì‚¬íšŒì  ê±°ë¦¬ (ì¦ê°€)
        'feat_power': 2.0,     # ê¶Œë ¥ê±°ë¦¬ (ì¦ê°€)
        'feat_indirect': 1.5,  # ê°„ì ‘ì„± (ì¦ê°€)
        'feat_strat_cnt': 1.0  # ì „ëµì  í‘œí˜„ (ê¸°ë³¸)
    }
    
    # í”¼ì²˜ë³„ ë¶„ë¥˜ ì†ì‹¤
    for i, feat_name in enumerate(feature_names):
        if feat_mask[:, i].any():  # í•´ë‹¹ í”¼ì²˜ê°€ ë§ˆìŠ¤í‚¹ë˜ì§€ ì•Šì€ ìƒ˜í”Œì´ ìˆëŠ” ê²½ìš°
            valid_mask = feat_mask[:, i]
            valid_logits = feature_logits[feat_name][valid_mask]
            valid_labels = feat_labels[valid_mask, i]
            
            if len(valid_logits) > 0:
                weights = class_weights_dict.get(feat_name, None)
                
                if use_focal:
                    focal_loss = FocalLoss(alpha=alpha, gamma=gamma, weight=weights)
                    feat_loss = focal_loss(valid_logits, valid_labels)
                else:
                    feat_loss = F.cross_entropy(valid_logits, valid_labels, weight=weights)
                
                # í”¼ì²˜ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©
                importance_weight = feature_importance.get(feat_name, 1.0)
                weighted_feat_loss = feat_loss * importance_weight
                
                total_loss += weighted_feat_loss
                
                # ë””ë²„ê¹…ìš© ì¶œë ¥ (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
                if torch.rand(1).item() < 0.01:  # 1% í™•ë¥ ë¡œ ì¶œë ¥
                    print(f"  {feat_name}: loss={feat_loss:.4f}, importance={importance_weight}, weighted={weighted_feat_loss:.4f}")
    
    # ìŠ¤ì½”ì–´ íšŒê·€ ì†ì‹¤ (ì¤‘ìš”ë„ ë†’ê²Œ)
    if score_mask.any():
        valid_score_pred = score_pred[score_mask]
        valid_score_labels = score_labels[score_mask]
        score_loss = F.mse_loss(valid_score_pred, valid_score_labels)
        
        # ìŠ¤ì½”ì–´ ì†ì‹¤ì€ ìµœì¢… ëª©í‘œì´ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜
        weighted_score_loss = score_loss * 2.0
        total_loss += weighted_score_loss
    
    return total_loss