#!/usr/bin/env python3
"""
í•œêµ­ì–´ ê³µì†ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
ëª¨ë¸ í›ˆë ¨ ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import pandas as pd
import torch
from transformers import BertTokenizer

def test_data_loading():
    """ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ğŸ“ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸...")
    
    try:
        # Manual ë°ì´í„° ì²´í¬
        manual_path = "./data/manual_labels.csv"
        if os.path.exists(manual_path):
            manual_df = pd.read_csv(manual_path)
            print(f"âœ… Manual ë°ì´í„°: {len(manual_df)}ê°œ ë¬¸ì¥ ë¡œë“œ ì„±ê³µ")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
            required_cols = ['sentence', 'feat_ending', 'feat_strat_cnt', 'feat_command', 
                           'feat_attack', 'feat_power', 'feat_distance', 'feat_indirect', 'score']
            missing_cols = [col for col in required_cols if col not in manual_df.columns]
            
            if missing_cols:
                print(f"âš ï¸  ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols}")
            else:
                print("âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬")
                
            # ê¸°ë³¸ í†µê³„
            print(f"   - ì ìˆ˜ ë²”ìœ„: {manual_df['score'].min():.1f} ~ {manual_df['score'].max():.1f}")
            print(f"   - í‰ê·  ì ìˆ˜: {manual_df['score'].mean():.2f}")
            
        else:
            print(f"âŒ Manual ë°ì´í„° íŒŒì¼ ì—†ìŒ: {manual_path}")
        
        # TyDiP ë°ì´í„° ì²´í¬
        tydip_path = "./data/ko_test.csv"
        if os.path.exists(tydip_path):
            tydip_df = pd.read_csv(tydip_path)
            print(f"âœ… TyDiP ë°ì´í„°: {len(tydip_df)}ê°œ ë¬¸ì¥ ë¡œë“œ ì„±ê³µ")
            print(f"   - ì ìˆ˜ ë²”ìœ„: {tydip_df['score'].min():.1f} ~ {tydip_df['score'].max():.1f}")
            print(f"   - í‰ê·  ì ìˆ˜: {tydip_df['score'].mean():.2f}")
        else:
            print(f"âŒ TyDiP ë°ì´í„° íŒŒì¼ ì—†ìŒ: {tydip_path}")
            
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")

def test_tokenizer():
    """í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”¤ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸...")
    
    try:
        tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.unk_token
        
        test_sentence = "ì•ˆë…•í•˜ì„¸ìš”. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        
        encoded = tokenizer(
            test_sentence,
            truncation=True,
            padding='max_length',
            max_length=256,
            return_tensors='pt'
        )
        
        print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        print(f"   - í…ŒìŠ¤íŠ¸ ë¬¸ì¥: {test_sentence}")
        print(f"   - í† í° ìˆ˜: {(encoded['attention_mask'] == 1).sum().item()}")
        print(f"   - Input shape: {encoded['input_ids'].shape}")
        
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ì˜¤ë¥˜: {e}")

def test_model_architecture():
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸...")
    
    try:
        from multitask_model import ImprovedMultiTaskModel, KoPolitenessDataset
        
        # ëª¨ë¸ ìƒì„± (ì‹¤ì œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì•ˆí•¨)
        model = ImprovedMultiTaskModel(
            model_name='monologg/kobert',
            dropout_rate=0.3
        )
        
        print(f"âœ… ëª¨ë¸ ìƒì„± ì„±ê³µ")
        print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   - í”¼ì²˜ í—¤ë“œ ìˆ˜: {len(model.feature_heads)}")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
        dummy_input_ids = torch.randint(0, 1000, (2, 256))  # batch_size=2, seq_len=256
        dummy_attention_mask = torch.ones(2, 256)
        
        with torch.no_grad():
            feature_logits, score_pred = model(dummy_input_ids, dummy_attention_mask)
        
        print(f"âœ… ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"   - í”¼ì²˜ ì¶œë ¥ ìˆ˜: {len(feature_logits)}")
        print(f"   - ì ìˆ˜ ì¶œë ¥ shape: {score_pred.shape}")
        
        # í”¼ì²˜ë³„ ì¶œë ¥ í˜•íƒœ í™•ì¸
        for feat_name, logits in feature_logits.items():
            print(f"   - {feat_name}: {logits.shape}")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì˜¤ë¥˜: {e}")

def test_dataset():
    """ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š ë°ì´í„°ì…‹ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸...")
    
    try:
        from multitask_model import KoPolitenessDataset
        from transformers import BertTokenizer
        
        tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.unk_token
        
        # ë”ë¯¸ ë°ì´í„°
        sentences = ["ì•ˆë…•í•˜ì„¸ìš”.", "ì•¼, ë­í•´?", "ì£„ì†¡í•©ë‹ˆë‹¤."]
        feat_labels = [[2, 1, 0, 0, 1, 2, 0], [0, 0, 0, 0, 0, 0, 0], [2, 1, 0, 0, 1, 2, 1]]
        scores = [1.5, -0.5, 1.0]
        
        # Manual ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
        dataset = KoPolitenessDataset(
            sentences=sentences,
            feat_labels=feat_labels,
            scores=scores,
            tokenizer=tokenizer,
            max_length=256
        )
        
        print(f"âœ… Manual ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ")
        print(f"   - ë°ì´í„° ìˆ˜: {len(dataset)}")
        
        # ìƒ˜í”Œ í™•ì¸
        sample = dataset[0]
        print(f"   - ìƒ˜í”Œ í‚¤: {list(sample.keys())}")
        print(f"   - Input shape: {sample['input_ids'].shape}")
        print(f"   - Feature labels: {sample['feat_labels']}")
        print(f"   - Score: {sample['score'].item():.2f}")
        
        # TyDiP ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (í”¼ì²˜ ë¼ë²¨ ì—†ìŒ)
        tydip_dataset = KoPolitenessDataset(
            sentences=sentences,
            feat_labels=None,
            scores=scores,
            tokenizer=tokenizer,
            max_length=256
        )
        
        tydip_sample = tydip_dataset[0]
        print(f"âœ… TyDiP ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ")
        print(f"   - Feature mask: {tydip_sample['feat_mask']}")  # ëª¨ë‘ Falseì—¬ì•¼ í•¨
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì˜¤ë¥˜: {e}")

def test_feature_distribution():
    """í”¼ì²˜ ë¶„í¬ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“ˆ í”¼ì²˜ ë¶„í¬ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
    
    try:
        manual_path = "./data/manual_labels.csv"
        if not os.path.exists(manual_path):
            print("âŒ Manual ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ë¶„í¬ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        manual_df = pd.read_csv(manual_path)
        feature_cols = [
            "feat_ending", "feat_strat_cnt", "feat_command",
            "feat_attack", "feat_power", "feat_distance", "feat_indirect"
        ]
        
        print("âœ… í”¼ì²˜ë³„ í´ë˜ìŠ¤ ë¶„í¬:")
        
        for col in feature_cols:
            if col in manual_df.columns:
                counts = manual_df[col].value_counts().sort_index()
                total = len(manual_df)
                
                print(f"\n   {col}:")
                for class_val, count in counts.items():
                    percentage = (count / total) * 100
                    print(f"     í´ë˜ìŠ¤ {class_val}: {count:3d}ê°œ ({percentage:5.1f}%)")
                
                # í¬ê·€ í´ë˜ìŠ¤ ê²½ê³ 
                rare_classes = counts[counts < 10].index.tolist()
                if rare_classes:
                    print(f"     âš ï¸  í¬ê·€ í´ë˜ìŠ¤ (< 10ê°œ): {rare_classes}")
            else:
                print(f"   âŒ {col} ì»¬ëŸ¼ ì—†ìŒ")
                
    except Exception as e:
        print(f"âŒ í”¼ì²˜ ë¶„í¬ ë¶„ì„ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª í•œêµ­ì–´ ê³µì†ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # GPU í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if torch.cuda.is_available():
        print(f"   - GPU: {torch.cuda.get_device_name(0)}")
        print(f"   - ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_data_loading()
    test_tokenizer()
    test_model_architecture()
    test_dataset()
    test_feature_distribution()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. python train_multitask.py - ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰")
    print("   2. python inference_utils.py - ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    print("   3. í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ ë¬¸ì¥ ë¶„ì„")

if __name__ == "__main__":
    main()