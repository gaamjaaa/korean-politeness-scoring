#!/usr/bin/env python3
"""
ì˜¬ë°”ë¥¸ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ (ë°ì´í„° ë¦¬í‚¤ì§€ ì œê±°):
í…ìŠ¤íŠ¸ â†’ í”¼ì²˜ ì¶”ì¶œ â†’ í”¼ì²˜ë³„ ì ìˆ˜ + ì „ì²´ ì ìˆ˜ ì˜ˆì¸¡
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import json
import os

def main():
    print("ğŸ”§ CORRECTED MULTITASK Korean Politeness Training")
    print("=" * 60)
    print("ğŸš¨ FIXING DATA LEAKAGE:")
    print("   âŒ Before: Manual features â†’ Manual features (LEAKAGE!)")
    print("   âœ… After: Text â†’ Extracted features â†’ Manual features")
    print()
    print("ğŸ“Š Target Tasks:")
    print("   1. feat_ending prediction")
    print("   2. feat_strat_cnt prediction") 
    print("   3. feat_indirect prediction")
    print("   4. feat_command prediction")
    print("   5. feat_attack prediction")
    print("   6. feat_power prediction")
    print("   7. feat_distance prediction")
    print("   8. Overall score prediction")
    
    # Manual Labels ë¡œë“œ
    manual_df = pd.read_csv('./data/manual_labels_new_system.csv')
    print(f"\nğŸ“Š Using Manual Labels: {len(manual_df)} samples")
    
    # í…ìŠ¤íŠ¸ì—ì„œ í”¼ì²˜ ì¶”ì¶œ (ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©)
    print(f"\nğŸ” Extracting features from text (INPUT)...")
    
    from preprocessing.korean_politeness_analyzer import KoreanPolitenessAnalyzer
    analyzer = KoreanPolitenessAnalyzer()
    
    feature_cols = ['feat_ending', 'feat_strat_cnt', 'feat_indirect', 
                   'feat_command', 'feat_attack', 'feat_power', 'feat_distance']
    
    # X: í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ í”¼ì²˜ë“¤ (ì…ë ¥)
    extracted_features = []
    for sentence in manual_df['sentence']:
        features = analyzer.extract_features(sentence)
        extracted_features.append([features[feat] for feat in feature_cols])
    
    X = np.array(extracted_features)
    
    # Y: Manual Labelsì˜ ground truth í”¼ì²˜ ì ìˆ˜ë“¤ + ì „ì²´ ì ìˆ˜ (íƒ€ê²Ÿ)
    target_cols = feature_cols + ['score']
    Y = manual_df[target_cols].values
    
    print(f"\nğŸ¯ Corrected Multitask Setup:")
    print(f"   Input: Extracted features from TEXT")
    print(f"   Output: Manual Labels ground truth scores")
    print(f"   Input Features: {X.shape[1]} features")
    print(f"   Output Targets: {Y.shape[1]} targets")
    print(f"   Target Tasks: {target_cols}")
    
    # ì…ë ¥ê³¼ ì¶œë ¥ í†µê³„ ë¹„êµ
    print(f"\nğŸ“Š Input vs Output Statistics:")
    for i, feat in enumerate(feature_cols):
        input_mean = np.mean(X[:, i])
        output_mean = np.mean(Y[:, i])
        print(f"   {feat}: Input={input_mean:.3f}, Target={output_mean:.3f}")
    
    # ì¸µí™”ë¶„í• ì„ ìœ„í•´ ì „ì²´ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    y_discrete = pd.cut(manual_df['score'], bins=5, labels=False)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # ë©€í‹°íƒœìŠ¤í¬ ëª¨ë¸
    multitask_model = MultiOutputRegressor(
        RandomForestRegressor(
            n_estimators=200, 
            max_depth=10, 
            min_samples_split=5,
            random_state=42
        )
    )
    
    print(f"\nğŸ”„ Starting 5-Fold Cross Validation...")
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_discrete)):
        print(f"\nğŸ“Š Fold {fold + 1}/5:")
        
        X_train, X_val = X[train_idx], X[val_idx]
        Y_train, Y_val = Y[train_idx], Y[val_idx]
        
        print(f"   Train: {len(X_train)} samples")
        print(f"   Val: {len(X_val)} samples")
        
        # í›ˆë ¨
        multitask_model.fit(X_train, Y_train)
        
        # ì˜ˆì¸¡
        Y_pred = multitask_model.predict(X_val)
        
        # ê° íƒœìŠ¤í¬ë³„ í‰ê°€
        task_results = {}
        
        for i, task in enumerate(target_cols):
            y_true = Y_val[:, i]
            y_pred = Y_pred[:, i]
            
            mae = mean_absolute_error(y_true, y_pred)
            r2 = r2_score(y_true, y_pred)
            
            task_results[task] = {'mae': mae, 'r2': r2}
            
            print(f"   {task}: MAE={mae:.4f}, RÂ²={r2:.4f}")
        
        fold_results.append(task_results)
    
    # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    print(f"\nğŸ† CORRECTED MULTITASK RESULTS (5-Fold CV):")
    print("=" * 60)
    
    avg_results = {}
    for i, task in enumerate(target_cols):
        maes = [fold[task]['mae'] for fold in fold_results]
        r2s = [fold[task]['r2'] for fold in fold_results]
        
        avg_mae = np.mean(maes)
        avg_r2 = np.mean(r2s)
        std_r2 = np.std(r2s)
        
        avg_results[task] = {
            'avg_mae': avg_mae,
            'avg_r2': avg_r2,
            'std_r2': std_r2
        }
        
        print(f"ğŸ“Š {task}:")
        print(f"   MAE: {avg_mae:.4f}")
        print(f"   RÂ²: {avg_r2:.4f} Â± {std_r2:.4f}")
        print()
    
    # ìµœì¢… ëª¨ë¸ í›ˆë ¨
    print(f"ğŸš€ Training final corrected multitask model...")
    multitask_model.fit(X, Y)
    
    # í”¼ì²˜ ì¤‘ìš”ë„ (ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ê¸°ì¤€)
    if hasattr(multitask_model.estimators_[0], 'feature_importances_'):
        importances = multitask_model.estimators_[0].feature_importances_
        print(f"\nğŸ¯ Feature Importance (feat_ending task):")
        for feat, imp in zip(feature_cols, importances):
            print(f"   {feat}: {imp:.4f}")
    
    # Ko Testë¡œ ì¼ë°˜í™” í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª Generalization Test on Ko Test...")
    
    ko_test_df = pd.read_csv('./data/ko_test.csv')
    
    ko_features = []
    for sentence in ko_test_df['sentence']:
        features = analyzer.extract_features(sentence)
        ko_features.append([features[feat] for feat in feature_cols])
    
    X_ko = np.array(ko_features)
    
    # ë©€í‹°íƒœìŠ¤í¬ ì˜ˆì¸¡
    Y_pred_ko = multitask_model.predict(X_ko)
    
    # Ko TestëŠ” ì „ì²´ ì ìˆ˜ë§Œ ìˆìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ íƒœìŠ¤í¬(score)ë§Œ í‰ê°€
    y_ko_true = ko_test_df['score'].values
    y_ko_pred = Y_pred_ko[:, -1]  # ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ score
    
    mae_ko = mean_absolute_error(y_ko_true, y_ko_pred)
    r2_ko = r2_score(y_ko_true, y_ko_pred)
    
    print(f"   Ko Test Overall Score - MAE: {mae_ko:.4f}, RÂ²: {r2_ko:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('./corrected_multitask_results', exist_ok=True)
    
    # ì„±ëŠ¥ ìš”ì•½
    summary = {
        'approach': 'Corrected Multitask Learning (No Data Leakage)',
        'description': 'Text â†’ Extracted features â†’ Feature scores + Overall score',
        'data_leakage_fixed': True,
        'input_source': 'Text-extracted features',
        'output_target': 'Manual Labels ground truth scores',
        'dataset_size': len(manual_df),
        'cv_results': avg_results,
        'ko_test_generalization': {
            'overall_score_mae': mae_ko,
            'overall_score_r2': r2_ko
        }
    }
    
    with open('./corrected_multitask_results/corrected_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    # Ko Test ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    ko_results_df = ko_test_df.copy()
    ko_results_df['predicted_score'] = y_ko_pred
    
    # ê° í”¼ì²˜ ì˜ˆì¸¡ê°’ë„ ì €ì¥
    for i, feat in enumerate(feature_cols):
        ko_results_df[f'predicted_{feat}'] = Y_pred_ko[:, i]
        ko_results_df[f'extracted_{feat}'] = X_ko[:, i]
    
    ko_results_df.to_csv('./corrected_multitask_results/ko_test_corrected_predictions.csv', index=False)
    
    print(f"\nğŸ’¾ Results saved to './corrected_multitask_results/'")
    
    # ì„±ëŠ¥ ë¹„êµ
    print(f"\nğŸ” DATA LEAKAGE IMPACT:")
    print(f"ğŸ“Š Previous (WITH LEAKAGE):")
    print(f"   feat_power: RÂ² 0.9988 (too perfect!)")
    print(f"   feat_ending: RÂ² 0.9947 (suspicious)")
    print(f"ğŸ“Š Current (CORRECTED):")
    print(f"   Overall Score RÂ²: {avg_results['score']['avg_r2']:.4f}")
    
    best_feature = max(
        [task for task in target_cols if task != 'score'], 
        key=lambda x: avg_results[x]['avg_r2']
    )
    worst_feature = min(
        [task for task in target_cols if task != 'score'], 
        key=lambda x: avg_results[x]['avg_r2']
    )
    
    print(f"   Best Feature: {best_feature} (RÂ² {avg_results[best_feature]['avg_r2']:.4f})")
    print(f"   Worst Feature: {worst_feature} (RÂ² {avg_results[worst_feature]['avg_r2']:.4f})")
    
    print(f"\nğŸ¯ Corrected Multitask Learning:")
    print(f"   âœ“ No data leakage")
    print(f"   âœ“ Text â†’ Features â†’ Scores")
    print(f"   âœ“ Realistic performance")
    print(f"   âœ“ Individual feature evaluation possible")

if __name__ == "__main__":
    main() 